---
title: "Econometrics Assignment 4"
author: "Chase Bookin"
date: "July 14, 2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidycensus)
library(ggthemes)
library(ggplot2)
library(janitor)
library(dplyr)
library(gt)
library(readxl)
library(sf)
library(scales)
library(magrittr)
library(haven)
library(infer)
library(Lahman)
library(xml2)
library(rvest)
library(devtools)
library(broom)
library(retrosheet)
library(skimr)
library(knitr)
library(tinytex)
library(lubridate)
library(pander)
library(foreign)
library(lmtest)
library(sandwich)
```

**1. **  
  
**A) **
```{r}
x <- 36.77097
null <- 0
p <- 0.048
p_div_2 <- p / 2
t <- 1.98
# t is positive because we are testing the coefficient of 36.77 against the
# hypothesis that it is zero.

robust_se <- (x - null) / t

```
  
To find the t-statistic and Robust Standard Error of the size coefficient, we
begin with the p-value provided by Stata. By dividng the p-value by 2 and using
a z-score table to match the new CDF value of 0.024, we arrive at the
t-statistic of 1.98. We know it is positive because the coefficient for size is
positive and being compared to the null hypothesis that the size coefficient is
equal to zero. Next, we plug in what we have to the t-stat formula. Dividing the
size coefficient of 37.77097 by the t value of 1.98 yields a Robust Standard
Error of 18.57. In summary, the t value is found to be 1.98 and the Robust
Standard Error is 18.57.

**B) **  

The slope coefficient on size decreased from the first regression to the second
regression because we extracted the effect of additional bathrooms on price, and
the number of bathrooms in a home is correlated to the size of the home.
Essentially, the first regression was overestimating the effect of home size on
price through omitted variable bias. Larger homes tend to have more bathrooms,
and more bathrooms make homes worth more. By omitting bathrooms in the first
regression, its effect was being grouped into the size of the house, yielding
the initial overestimate of the first regression and subsequent drop in the size
coefficient in the multivariate regression.  
  

**2. **  
  
**B) **  

```{r, echo=FALSE}
crime2 <- read_xlsx("crime.xlsx") %>% 
  rename("crime_rate" = crmrte,
         "arrest_prob" = prbarr,
         "convict_prob" = prbconv,
         "prison_prob" = prbpris,
         "mean_sentence" = avgsen,
         "police_per_cap" = polpc,
         "tax_per_cap" = taxpc,
         "minority_percent" = pctmin80,
         "young_male_percent" = pctymle,
         "wage_construction" = wcon,
         "wage_tuc" = wtuc,
         "wage_trd" = wtrd,
         "wage_fir" = wfir,
         "wage_ser" = wser,
         "wage_mfg" = wmfg,
         "wage_fed" = wfed,
         "wage_sta" = wsta,
         "wage_loc" = wloc)

crime_summary <- read_xlsx("crime.xlsx") %>% 
  rename("Crime Rate" = crmrte,
         "Arrest Probability" = prbarr,
         "Conviction Prob" = prbconv,
         "Prison Prob" = prbpris,
         "Average Sentence" = avgsen,
         "Police per Capita" = polpc,
         "Tax Revenue per Cap" = taxpc,
         "Percent Minority" = pctmin80,
         "Percent Young Male" = pctymle,
         "Wage: Construction" = wcon,
         "Wage: Tran & Util" = wtuc,
         "Wage: Trade" = wtrd,
         "Wage: Finance" = wfir,
         "Wage: Service" = wser,
         "Wage: Manufacturing" = wmfg,
         "Wage: Federal Gov" = wfed,
         "Wage: State Gov" = wsta,
         "Wage: Local Gov" = wloc)

crime_summary %>% 
  skim_without_charts()
```

COLE COMMENT ABOVE
COLE COMMENT ABOVE
COLE COMMENT ABOVE
  
  
**C) **  
```{r, echo=FALSE}
  
mean_crime <- mean(crime2$crime_rate)

crime2 %>%
  ggplot(aes(x = crime_rate)) +
  geom_histogram(bins = 50, color = "black", fill = "dodgerblue3") +
  theme_economist() +
  scale_x_continuous(breaks = c(0, .025, .05, .075, .1, .125)) +
  scale_y_continuous(breaks = c(0, 2, 4, 6, 8, 10, 12, 14)) +
  labs(
    title = "Distribution of Crime Rates",
    subtitle = "Using data from North Carolina counties in 1987",
    x = "Crime Rate",
    y = "Frequency"
  ) +
  geom_vline(xintercept = 0.0335, linetype = "longdash", color = "black") +
  annotate("text", x = .05, y = 9, label = "Mean Crime Rate: 0.0335")
```
COLE COMMENT
COLE COMMENT
COLE COMMENT

```{r, echo=FALSE, warning=FALSE}
mean_arrest <- mean(crime2$arrest_prob)

crime2 %>%
  filter(!is.na(arrest_prob)) %>% 
  ggplot(aes(x = arrest_prob)) +
  geom_histogram(bins = 40, color = "black", fill = "dodgerblue3") +
  theme_economist() +
  scale_x_continuous(limits = c(0,125), breaks = c(0, 25, 50, 75, 100, 125)) +
  scale_y_continuous(breaks = c(0, 3, 6, 9, 12)) +
  labs(
    title = "Distribution of Arrest Percentages for Offenses",
    subtitle = "Using data from North Carolina counties in 1987",
    x = "Arrest Percentage",
    y = "Frequency"
  ) +
  geom_vline(xintercept = 29.52375, linetype = "longdash", color = "black") +
  annotate("text", x = 54, y = 11.5, label = "Mean Arrest Percent: 29.52%")
```

COLE COMMENT
COLE COMMENT
COLE COMMENT  

**D) **
```{r, echo=FALSE}

variables <- c("crime_rate", "arrest_prob", "convict_prob", "prison_prob", "mean_sentence", "police_per_cap")

corr <- crime2 %>% 
  select(crime_rate, arrest_prob, convict_prob, prison_prob, mean_sentence, police_per_cap) %>% 
  filter(!is.na(arrest_prob) & !is.na(convict_prob)) %>% 
  cor() %>% 
  round(2)

correlation_table <- as_tibble(corr) %>% 
  mutate(' ' = variables) %>% 
  select(' ', everything())


gt(data = correlation_table) %>% 
  tab_header(
    title = "Correlation Matrix",
    subtitle = "Correlations of crime rate and possible explanatory variables"
  ) %>% 
  tab_source_note(
    source_note = "Data collected from North Carolina Counties in 1987"
  )

```

**E) **

```{r, echo=FALSE}
model1 <- lm(crime_rate ~ arrest_prob, data = crime2)

model1_tidy <- model1 %>%
  tidy()

model1_robust <- coeftest(model1, vcov = vcovHC(model1, type="HC1")) %>% 
  tidy()

gt <- gt(data=as_tibble(model1_robust)) %>% 
  tab_header(
    title = "Regression Output of Crime Rate on Probability of Arrest",
    subtitle = "Using HC1 Robust Standard Errors"
  ) %>% 
  tab_source_note(
    source_note = "Data collected from North Carolina Counties in 1987"
  )

model1_robust

```
  
The regression coefficient of the arrest probability is -0.000542. This means that if arrest probability increases by one percentage point, we expect the crimes committed per person to decrease by 0.000542.
  
 
```{r, include=FALSE}
zillow <- .01/.0005422
intercept <- model1_robust %>% 
  filter(term == '(Intercept)') %>% 
  pull(estimate)

b1 <- model1_robust %>% 
  filter(term == 'arrest_prob') %>% 
  pull(estimate)

# y = intercept + b1 * arrest_prob

((.02 - intercept) / b1) - ((.03 - intercept) / b1)

```
 
    
**F) **

```{r}
estimate <- -0.0005422804
null <- 0
se <- 0.0001278931

t <- (estimate - null) / se
```
  
From the above calculations as well as the regression output, we find the
absolute value of the t-statistic of the regression coefficient for arrest
probability to be 4.24. Because the absolute value of the t-statistic is greater
than the critical value of 1.96 at a 5% significance level, we reject the null
hypothesis that the coefficient for arrest probability is equal to zero.
  
However, from a practical significance perspective, assuming a population of one
million people, an increase of one percent in arrest probability would only drop
the total amount of crimes committed by 542 based on the arrest probability
coefficient. Most counties in North Carolina do not have one million people,
meaning that total crime reduction would be even smaller in these counties.
  
Additionally, the 50th percentile for crime rate is 0.03. Based on the
regression, in order to move from the 50th percentile to the 25th percentile
crime rate of 0.02, it is estimated that a county would need to increase its
arrest probability by roughly 18.44%, a major increase. This exemplifies the lack of
practical significance of the arrest probability coefficient, despite its
statistical significance.  
  
**G) **

```{r}
intercept <- model1_robust %>% 
  filter(term == '(Intercept)') %>% 
  pull(estimate)

b1 <- model1_robust %>% 
  filter(term == 'arrest_prob') %>% 
  pull(estimate)

# y = intercept + b1 * arrest_prob

est_arrest_03 <- (.03 - intercept) / b1 # approx 35.996

est_arrest_02 <- (.02 - intercept) / b1 # approx 54.437

est_arrest_02 - est_arrest_03
```
  
Assuming a county in North Carolina wants to drop from 5 crimes per 100 citizens to 4 crimes per 100 citizens, the estimated increase in arrest probability necessary would be roughly 18.44%. This is the same as the example explained in part F because in each instance the Y variable is being dropped by 0.01 and we are using a linear regression, meaning the slope is the same at all points. Given this, we took the expected arrest probabilities for crime rates of 0.03 and 0.02 and found the difference between them to be 18.44%. This estimated increase in arrest probability required is equal to the change required to drop from 0.05 to 0.04, leaving us with the answer of 18.44%.  
  
**H) **

```{r, echo=FALSE}

model1_fitted <- model1 %>%
  augment(type.predict = "response", data = crime2) %>% 
  select(county, crime_rate, arrest_prob, .fitted, .resid)

model1_fitted %>% 
  ggplot(aes(x = arrest_prob, y = .resid)) +
  geom_point(color = "dodgerblue3") +
  theme_economist() +
  scale_x_continuous(limits = c(0,125), breaks = c(0, 25, 50, 75, 100, 125)) +
  scale_y_continuous(limits = c(-.04, .06), breaks = c(-.04, -.02, 0, .02, .04, .06)) +
  labs(
    title = "Arrest Probability vs Residuals",
    x = "Arrest Probability",
    y = "Residuals",
    caption = "Data collected from North Carolina Counties in 1987"
  )

```
  
Based on this graph, it is clear that the heteroskedasticity-robust estimator for standard error is justified. The residuals have much greater spread for counties with lower arrest probabilities compared to counties with greater arrest probabilities. Because the residuals are not uniformly distributed along the x axis, we need to use the heteroskedasticity-robust standard error.  

**I) **



